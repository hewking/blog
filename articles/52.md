# llm 原理

> Author: **hewking**  
> Labels: **blog**  
> Created: **2025-02-13T13:44:25Z**  
> Link and comments: <https://github.com/hewking/blog/issues/52>  


# llm 原理

## LLM 预训练

步骤：

1. 数据集
2. token 序列化
3. 预训练：神经网络训练
4. 推理

## AI News

1. [AI News Newsletter](https://buttondown.com/ainews)
2. [AHEAD OF Ai](https://magazine.sebastianraschka.com)
3. [lilianweng blogs](https://lilianweng.github.io/archives/)

## 参考

1. [Andrej Karpatht: Deep Dive into LLMs like ChatGPT ](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=10913s)
   1. [HuggingFaceFW/fineweb: 数据集](https://huggingface.co/datasets/HuggingFaceFW/fineweb)
   2. [Transformer Neural Net 3D visualizer LLM 可视化](https://bbycroft.net/llm)
   3. [token 序列化示例](https://tiktokenizer.vercel.app/)
   4. [karpathy 复现 GPT-2 示例](https://github.com/karpathy/llm.c/discussions/677)
   5. [Llama 3 paper from Meta](https://arxiv.org/abs/2407.21783)
   6. [hyperbolic: 云端大模型 demo 和 api](https://app.hyperbolic.xyz/models)
   7. [HuggingFace inference playground](https://huggingface.co/spaces/huggingface/inference-playground)
   8. [DeepSeek-R1 paper](https://arxiv.org/abs/2501.12948)
   9. [TogetherAI Playground for open model inference](https://api.together.xyz/playground)
   10. [LM Arena for model rankings](https://lmarena.ai/)
   11. [AI News Newsletter](https://buttondown.com/ainews)
   12. [LMStudio for local inference](https://lmstudio.ai/)
2. [最好的致敬是学习：DeepSeek-R1 赏析](https://www.bilibili.com/video/BV1bnNDeFELK/?spm_id_from=333.337.search-card.all.click&vd_source=372969eee60ab0f69d248a32e78e79b7)
   1. ![ R1 训练流程图](https://cdn.jsdelivr.net/gh/hewking/myrepo/image/20250212213933.png)
   2. ![R1 训练流程图@刘囧](https://cdn.jsdelivr.net/gh/hewking/myrepo/image/20250212214018.png)
   3. [Understanding Reasoning LLMs](https://magazine.sebastianraschka.com/p/understanding-reasoning-llms)
3. [Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)
   1. [karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)
   2. [The Transformer Family](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/)
   3. [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
   4. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
4. [Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g)
   1. 模型文件本质：参数文件（pytorch: .pt 文件） + run.c 运行代码
   2. 参数文件：有损压缩互联网数据为神经网络
   3. 神经网络：数据之间会形成规律
   4. 预训练：BaseModal
   5. 微调: Assistant Modal
   6. 模型的幻觉：（模型输出时，类似在做梦）
   7. RL: HFRL, RL
   8. Thingking , System1, System2
   9. 多模态
   10. Scaling Law
   11. 大模型的未来
   12. 模型安全
   13. 视频资源
       1. [karpathy/llama2.c](https://github.com/karpathy/llama2.c/blob/master/run.c)
5. Tramsforms 原理
   1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
   2. [Attention in transformers, step-by-step | DL6](https://www.youtube.com/watch?v=eMlx5fFNoYc)
   3. [Visualizing transformers and attention | Talk for TNG Big Tech Day '24](https://www.youtube.com/watch?v=KJtZARuO3JY)
6. Understanding DeepSeek-R1
   1. [GRPO 是如何工作的？](https://www.youtube.com/watch?v=iHlarYGLMbY)
   2. [Transformer 论文逐段精读【论文精读】](https://www.bilibili.com/video/BV1pu411o7BE/?vd_source=372969eee60ab0f69d248a32e78e79b7)
